# H200 Trial Config for O-JEPA
  # Target: ~35M parameters on H200 (80GB VRAM)
  # Dataset: LUMA vision (~35K samples)
  # Expected training time: ~15-20 minutes

  model:
    byte_encoder:
      hidden_dim: 384
      num_layers: 2
      num_heads: 8
      max_seq_len: 8192

    backbone:
      hidden_dim: 384
      num_layers: 12
      num_heads: 8

    predictor:
      hidden_dim: 384
      num_layers: 4
      num_heads: 8
      output_dim: 384

    masking:
      num_target_blocks: 4
      target_scale_min: 0.15
      target_scale_max: 0.25
      context_scale_min: 0.85
      context_scale_max: 1.0

    ema:
      ema_decay_min: 0.996
      ema_decay_max: 0.9999
      ema_warmup_steps: 500

  training:
    # H200 has 80GB - go big
    batch_size: 128
    gradient_accumulation_steps: 2
    # effective_batch = 128 * 2 = 256

    # 2K steps = ~15 epochs (good trial)
    total_steps: 2000

    # Optimizer
    use_muon: true
    muon_lr: 1.5e-2
    adamw_lr: 5e-4
    weight_decay: 0.05

    # Scheduler (faster warmup for shorter run)
    warmup_steps: 200

    # Regularization
    dropout: 0.1

    # Logging (more frequent for trial)
    log_every: 50
    eval_every: 500
    save_every: 1000

  data:
    modality: vision
    vision_seq_len: 3072
    image_size: [32, 32]